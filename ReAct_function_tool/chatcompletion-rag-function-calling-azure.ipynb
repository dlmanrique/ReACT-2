{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to use functions with a knowledge base\n",
    "\n",
    "This notebook builds on the concepts in the [argument generation](How_to_call_functions_with_chat_models.ipynb) notebook, by creating an agent with access to a knowledge base and two functions that it can call based on the user requirement.\n",
    "\n",
    "We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\n",
    "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
    "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n",
    "\n",
    "This will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second.\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "This cookbook takes you through the following workflow:\n",
    "\n",
    "- **Search utilities:** Creating the two functions that access arXiv for answers.\n",
    "- **Configure Agent:** Building up the Agent behaviour that will assess the need for a function and, if one is required, call that function and present results back to the agent.\n",
    "- **arXiv conversation:** Put all of this together in live conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e71f33",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tiktoken (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [37 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "      copying tiktoken\\core.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "      copying tiktoken\\load.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "      copying tiktoken\\model.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "      copying tiktoken\\registry.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "      copying tiktoken\\__init__.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "      creating build\\lib.win-amd64-cpython-312\\tiktoken_ext\n",
      "      copying tiktoken_ext\\openai_public.py -> build\\lib.win-amd64-cpython-312\\tiktoken_ext\n",
      "      running egg_info\n",
      "      writing tiktoken.egg-info\\PKG-INFO\n",
      "      writing dependency_links to tiktoken.egg-info\\dependency_links.txt\n",
      "      writing requirements to tiktoken.egg-info\\requires.txt\n",
      "      writing top-level names to tiktoken.egg-info\\top_level.txt\n",
      "      reading manifest file 'tiktoken.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching 'Makefile'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'tiktoken.egg-info\\SOURCES.txt'\n",
      "      copying tiktoken\\py.typed -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tiktoken\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tiktoken)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy --quiet\n",
    "!pip install tenacity --quiet\n",
    "!pip install tiktoken==0.3.3 --quiet\n",
    "!pip install termcolor --quiet\n",
    "!pip install openai --quiet\n",
    "!pip install arxiv --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install PyPDF2 --quiet\n",
    "!pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from openai import AzureOpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "GPT_MODEL = \"gpt-4o-cde-aia\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002-cde-aia\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_BASE_2\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY_2\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION_2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e47962",
   "metadata": {},
   "source": [
    "## Search utilities\n",
    "\n",
    "We'll first set up some utilities that will underpin our two functions.\n",
    "\n",
    "Downloaded papers will be stored in a directory (we use ```./data/papers``` here). We create a file ```arxiv_library.csv``` to store the embeddings and details for downloaded papers to retrieve against using ```retrieve_related_doc_summary```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de5d32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './data/papers' already exists.\n"
     ]
    }
   ],
   "source": [
    "directory = './data/papers'\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(directory):\n",
    "    # If the directory doesn't exist, create it and any necessary intermediate directories\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully.\")\n",
    "else:\n",
    "    # If the directory already exists, print a message indicating it\n",
    "    print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5cb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57217b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query = \"quantum\",\n",
    "        max_results = 10,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in client.results(search):\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response.data[0].embedding,\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda02bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Core-hole Coherent Spectroscopy in Molecules',\n",
       " 'summary': 'We study the ultrafast dynamics initiated by a coherent superposition of\\ncore-excited states of nitrous oxide molecule. Using high-level\\n\\\\textit{ab-initio} methods, we show that the decoherence caused by the\\nelectronic decay and the nuclear dynamics is substantially slower than the\\ninduced ultrafast quantum beatings, allowing the system to undergo several\\noscillations before it dephases. We propose a proof-of-concept experiment using\\nthe harmonic up-conversion scheme available at X-ray free-electron laser\\nfacilities to trace the evolution of the created core-excited-state coherence\\nthrough a time-resolved X-ray photoelectron spectroscopy.',\n",
       " 'article_url': 'http://dx.doi.org/10.1103/PhysRevLett.132.263202',\n",
       " 'pdf_url': 'http://arxiv.org/abs/2406.19387v1'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11675627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7211df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages[0:1]:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def retrieve_related_doc_summary(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "898b94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n",
      "### Core Argument\n",
      "- The paper \"Enhancing Quantum State Discrimination with Indefinite Causal Order\" by Spiros Kechrimparis et al. argues that using protocols based on indefinite causal order, such as the quantum switch and its higher-order generalizations (superswitches), can significantly improve the process of quantum state discrimination in noisy channels.\n",
      "\n",
      "### Evidence\n",
      "- **Quantum State Discrimination**:\n",
      "  - Essential for applications in quantum communication, cryptography, and machine learning.\n",
      "  - The challenge is due to the non-orthogonality of quantum states, making perfect discrimination impossible.\n",
      "- **Noisy Channels**:\n",
      "  - Real-world quantum channels are noisy and often unknown, complicating state discrimination.\n",
      "  - Previous studies indicate that certain protocols can enhance guessing probability even with noise.\n",
      "- **Quantum Switch and Superswitches**:\n",
      "  - Introduction of the quantum switch, a supermap that superposes the sequential action of two quantum channels, leading to indefinite causal order.\n",
      "  - Proposal of higher-order generalizations called \"superswitches\" to further improve guessing probability.\n",
      "- **Advantages of Indefinite Causal Order**:\n",
      "  - Quantum switch and superswitches can significantly enhance guessing probability compared to traditional methods.\n",
      "  - These protocols can sometimes eliminate the need for redesigning optimal measurements for unknown states.\n",
      "- **Experimental and Theoretical Insights**:\n",
      "  - Review of existing literature on quantum state discrimination and the quantum switch.\n",
      "  - Detailed analysis of guessing probability for various ensembles of states and channels, comparing it to standard bounds.\n",
      "  - Extension to general Pauli channels and higher state-space dimensions, demonstrating the potential of superswitches in improving state discrimination.\n",
      "\n",
      "### Conclusions\n",
      "- The use of indefinite causal order, through quantum switches and superswitches, presents a promising approach to enhance quantum state discrimination in noisy channels.\n",
      "- This method can significantly improve the guessing probability, which is crucial for practical quantum communication and information processing tasks.\n",
      "- The findings suggest that these protocols could reduce the need for redesigning optimal measurements for unknown states, thereby simplifying the state discrimination process in practical applications.\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_related_doc_summary(\"PPO reinforcement learning sequence generation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab07e98",
   "metadata": {},
   "source": [
    "## Configure Agent\n",
    "\n",
    "We'll create our agent in this step, including a ```Conversation``` class to support multiple turns with the API, and some Python functions to enable interaction between the ```ChatCompletion``` API and our knowledge base functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a6fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, tools=None, model=GPT_MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",  # auto is default, but we'll be explicit, can be required if it must call one or more tools vía {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}, None if no calls are needed\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f7672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"tool\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978b7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"doc_retrievalaugmented\",\n",
    "            \"description\": \"\"\"Use this function to retrieve information usefull for you to answer the user question or query.\"\"\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the information required to answer a question in plain text based on the user's question or query. If the user's question or query is too complex this input should be a decomposition of the original user question focused on a specific single piece of information.\"\n",
    "                    }\n",
    "                },\n",
    "            \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "available_tools = {\n",
    "            \"doc_retrievalaugmented\": retrieve_related_doc_summary\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c88ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_tool_execution(messages, tools=[None], available_tools=available_tools):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding tools\"\"\"\n",
    "    \n",
    "    response = chat_completion_request(messages, tools)\n",
    "    print(\"first response\", response)\n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "    tool_calls = response_message.tool_calls\n",
    "    \n",
    "    if tool_calls:\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_tools[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            print(f\"Tool requested, calling function: \"+ str(function_name))\n",
    "            function_response = function_to_call(\n",
    "                query=function_args.get(\"query\")\n",
    "            )\n",
    "            \n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "            \n",
    "        response = chat_completion_request(messages, tools )\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e7868",
   "metadata": {},
   "source": [
    "## arXiv conversation\n",
    "\n",
    "Let's put this all together by testing our functions out in conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You retrieve the papers summaries to answer the customers questions.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253fd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first response ChatCompletion(id='chatcmpl-9fDIN0XqyXgk642NEzYBZbOzSbdRn', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_EG8wX9mC7fGbkCy9VTVipBla', function=Function(arguments='{\"query\":\"How does PPO reinforcement learning work?\"}', name='doc_retrievalaugmented'), type='function')]), content_filter_results={})], created=1719609583, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_abc28019ad', usage=CompletionUsage(completion_tokens=24, prompt_tokens=149, total_tokens=173), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "Tool requested, calling function: doc_retrievalaugmented\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Proximal Policy Optimization (PPO) is a type of reinforcement learning algorithm that improves upon previous methods by balancing ease of implementation and sample efficiency. Here’s a high-level explanation of how PPO works:\n",
       "\n",
       "1. **Policy Representation**: PPO maintains a policy, typically represented as a neural network, which maps states to actions probabilities. It also maintains a value function to estimate the expected return (value) for each state.\n",
       "\n",
       "2. **Interaction with the Environment**: The agent interacts with the environment by following the current policy to collect trajectories (sequences of states, actions, and rewards).\n",
       "\n",
       "3. **Advantage Estimation**: For each state-action pair in the trajectories, PPO estimates the \"advantage\", which is a measure of how much better the action taken was compared to the average action at that state. This helps in stabilizing learning.\n",
       "\n",
       "4. **Clipped Surrogate Objective**: PPO modifies the objective function of previous policy gradient methods to include a clipping term. This term ensures that the new policy doesn't deviate too much from the old policy, preventing large and potentially harmful updates. The clipped objective function is:\n",
       "   \\[\n",
       "   \\max\\left(\\mathbb{E}_t\\left[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} \\hat{A}_t\\right], \\mathbb{E}_t\\left[\\text{clip}\\left(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_t\\right]\\right)\n",
       "   \\]\n",
       "   where \\(\\hat{A}_t\\) is the estimated advantage, \\(\\pi_{\\theta}\\) and \\(\\pi_{\\theta_{\\text{old}}}\\) are the new and old policies, respectively, and \\(\\epsilon\\) is a small hyperparameter that defines the clipping range.\n",
       "\n",
       "5. **Optimization**: The network parameters are updated using gradient descent, optimizing the clipped surrogate objective. This allows the policy to improve over time while ensuring that updates are not too aggressive (due to the clipping).\n",
       "\n",
       "6. **Value Function Update**: Alongside, the value function is also updated by minimizing the mean squared error between the predicted value and the observed return.\n",
       "\n",
       "7. **Epochs and Mini-batches**: Instead of using single updates, PPO uses multiple epochs of mini-batch updates to improve the stability of learning.\n",
       "\n",
       "In summary, PPO strikes a balance between simplicity and robustness, providing a method that is easier to implement than Trust Region Policy Optimization (TRPO) but more effective than vanilla policy gradients. The core idea is to ensure stable updates by clipping the policy objective and using multiple epochs of mini-batch updates."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
    "chat_response = chat_completion_with_tool_execution(\n",
    "    paper_conversation.conversation_history, tools=rag_tools\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message.content\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ca3e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first response ChatCompletion(id='chatcmpl-9fDJ11nNQLuDcmsFO3yRb3fL1KKLw', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_EG8wX9mC7fGbkCy9VTVipBla', function=Function(arguments='{\"query\": \"Difference between PPO reinforcement learning and Quantum contextuality\"}', name='doc_retrievalaugmented'), type='function'), ChatCompletionMessageToolCall(id='call_vz2egpojISU88GFhMcvg6Vuh', function=Function(arguments='{\"query\": \"What is Quantum contextuality?\"}', name='doc_retrievalaugmented'), type='function')]), content_filter_results={})], created=1719609623, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_abc28019ad', usage=CompletionUsage(completion_tokens=65, prompt_tokens=1153, total_tokens=1218), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "Tool requested, calling function: doc_retrievalaugmented\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n",
      "Tool requested, calling function: doc_retrievalaugmented\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Difference between PPO Reinforcement Learning and Quantum Contextuality\n",
       "\n",
       "#### PPO Reinforcement Learning\n",
       "\n",
       "**Core Argument**: Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed to optimize a policy to balance exploration and exploitation effectively. It aims to improve decision-making in uncertain environments by ensuring updates are stable and efficient.\n",
       "\n",
       "**How It Works**:\n",
       "- **Policy Representation**: Uses a neural network to map states to action probabilities.\n",
       "- **Interaction with the Environment**: The agent collects data by interacting with the environment.\n",
       "- **Advantage Estimation**: Calculates the advantage to measure action quality.\n",
       "- **Clipped Surrogate Objective**: Uses a modified objective function with a clipping term to ensure updates don't deviate too much from the current policy.\n",
       "- **Optimization**: Updates the policy using gradient descent on the clipped surrogate objective.\n",
       "- **Value Function Update**: Minimizes the error in value estimates.\n",
       "- **Epochs and Mini-Batches**: Utilizes multiple epochs of mini-batch updates for stability.\n",
       "\n",
       "**Conclusions**: PPO is effective for training agents in complex environments, offering robustness and scalability for reinforcement learning tasks.\n",
       "\n",
       "#### Quantum Contextuality\n",
       "\n",
       "**Core Argument**: Quantum contextuality is a fundamental aspect of quantum mechanics where the outcome of a measurement cannot be explained by any non-contextual hidden variable theory. It highlights the non-classical nature of quantum measurements.\n",
       "\n",
       "**How It Works**:\n",
       "- **Quantum Measurements**: The outcome depends on the context, meaning the set of other measurements that could be performed alongside it.\n",
       "- **Experimentation and Models**: Demonstrated through various experiments and theoretical constructs that show measurement outcomes are not independent of other compatible measurements.\n",
       "\n",
       "**Evidence and Findings**:\n",
       "- **Quantum State Discrimination**: Techniques like quantum switches and superswitches (higher-order superpositions) can improve quantum state discrimination in noisy channels.\n",
       "- **Indefinite Causal Order**: Using quantum switches allows superposing the sequence of quantum operations, enhancing performance in tasks like state discrimination.\n",
       "\n",
       "**Conclusions**: Quantum contextuality is essential to understanding and leveraging the peculiarities of quantum mechanics, with significant implications for quantum computation and information theory.\n",
       "\n",
       "### Summary\n",
       "\n",
       "- **Domain**: \n",
       "  - PPO Reinforcement Learning belongs to **machine learning**, focusing on optimizing decision-making processes.\n",
       "  - Quantum Contextuality is a concept in **quantum mechanics**, dealing with the non-classical outcomes of quantum measurements.\n",
       "\n",
       "- **Problem Type**:\n",
       "  - PPO addresses policy optimization for agents interacting with uncertain environments.\n",
       "  - Quantum Contextuality addresses the fundamental nature of quantum measurements and their dependence on measurement context.\n",
       "\n",
       "Both concepts are pivotal in their respective fields but address very different types of problems—PPO in optimizing machine learning algorithms, and Quantum Contextuality in elucidating the fundamental behaviors of quantum systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Now, I want to know the difference between PPO reinforcement learning and Quantum contextuality\",\n",
    ")\n",
    "updated_response = chat_completion_with_tool_execution(\n",
    "    paper_conversation.conversation_history, tools=rag_tools\n",
    ")\n",
    "display(Markdown(updated_response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da9c93bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\\nYou retrieve the papers summaries to answer the customers questions.\\nBegin!'},\n",
       " {'role': 'user', 'content': 'Hi, how does PPO reinforcement learning work?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_EG8wX9mC7fGbkCy9VTVipBla', function=Function(arguments='{\"query\":\"How does PPO reinforcement learning work?\"}', name='doc_retrievalaugmented'), type='function')]),\n",
       " {'tool_call_id': 'call_EG8wX9mC7fGbkCy9VTVipBla',\n",
       "  'role': 'tool',\n",
       "  'name': 'doc_retrievalaugmented',\n",
       "  'content': \"### Core Argument\\n- The paper presents a quantum algorithm that achieves a nearly quartic speedup for the Planted Noisy kXOR problem, a significant improvement over classical algorithms.\\n- The algorithm is resource-efficient, requiring only a logarithmic number of qubits.\\n- The approach generalizes and simplifies previous work on the Tensor Principal Component Analysis (PCA) problem using the Kikuchi Method.\\n- The speedup is due to the efficient solution of the Guided Sparse Hamiltonian problem, which is naturally instantiated by planted inference problems.\\n- The results have cryptographic implications, suggesting vulnerabilities in certain cryptographic constructions to super-quadratic quantum attacks.\\n\\n### Evidence\\n- **Quantum Speedup**: Demonstrated nearly quartic speedup over classical algorithms for the Planted Noisy kXOR problem.\\n- **Logarithmic Qubit Usage**: The algorithm's efficiency in terms of quantum hardware is highlighted by its minimal qubit requirements.\\n- **Generalization and Simplification**: The authors build on Hastings' work on Tensor PCA, using the Kikuchi Method to achieve the speedup.\\n- **Guided Sparse Hamiltonian Problem**: The algorithm's efficiency is attributed to its effective handling of the Guided Sparse Hamiltonian problem.\\n- **Cryptographic Implications**: The potential vulnerability of cryptographic constructions using the Planted Noisy kXOR problem to quantum attacks is discussed.\\n\\n### Conclusions\\n- The proposed quantum algorithm represents a significant advancement in computational efficiency for planted inference problems.\\n- The resource efficiency of the algorithm makes it practical for implementation on quantum hardware.\\n- The general framework and simplification of previous methods provide a robust foundation for further research and application.\\n- The cryptographic implications highlight the need for reassessment of certain cryptographic constructions in the context of quantum computing advancements.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Proximal Policy Optimization (PPO) is a type of reinforcement learning algorithm that improves upon previous methods by balancing ease of implementation and sample efficiency. Here’s a high-level explanation of how PPO works:\\n\\n1. **Policy Representation**: PPO maintains a policy, typically represented as a neural network, which maps states to actions probabilities. It also maintains a value function to estimate the expected return (value) for each state.\\n\\n2. **Interaction with the Environment**: The agent interacts with the environment by following the current policy to collect trajectories (sequences of states, actions, and rewards).\\n\\n3. **Advantage Estimation**: For each state-action pair in the trajectories, PPO estimates the \"advantage\", which is a measure of how much better the action taken was compared to the average action at that state. This helps in stabilizing learning.\\n\\n4. **Clipped Surrogate Objective**: PPO modifies the objective function of previous policy gradient methods to include a clipping term. This term ensures that the new policy doesn\\'t deviate too much from the old policy, preventing large and potentially harmful updates. The clipped objective function is:\\n   \\\\[\\n   \\\\max\\\\left(\\\\mathbb{E}_t\\\\left[\\\\frac{\\\\pi_{\\\\theta}(a_t|s_t)}{\\\\pi_{\\\\theta_{\\\\text{old}}}(a_t|s_t)} \\\\hat{A}_t\\\\right], \\\\mathbb{E}_t\\\\left[\\\\text{clip}\\\\left(\\\\frac{\\\\pi_{\\\\theta}(a_t|s_t)}{\\\\pi_{\\\\theta_{\\\\text{old}}}(a_t|s_t)}, 1 - \\\\epsilon, 1 + \\\\epsilon\\\\right) \\\\hat{A}_t\\\\right]\\\\right)\\n   \\\\]\\n   where \\\\(\\\\hat{A}_t\\\\) is the estimated advantage, \\\\(\\\\pi_{\\\\theta}\\\\) and \\\\(\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\) are the new and old policies, respectively, and \\\\(\\\\epsilon\\\\) is a small hyperparameter that defines the clipping range.\\n\\n5. **Optimization**: The network parameters are updated using gradient descent, optimizing the clipped surrogate objective. This allows the policy to improve over time while ensuring that updates are not too aggressive (due to the clipping).\\n\\n6. **Value Function Update**: Alongside, the value function is also updated by minimizing the mean squared error between the predicted value and the observed return.\\n\\n7. **Epochs and Mini-batches**: Instead of using single updates, PPO uses multiple epochs of mini-batch updates to improve the stability of learning.\\n\\nIn summary, PPO strikes a balance between simplicity and robustness, providing a method that is easier to implement than Trust Region Policy Optimization (TRPO) but more effective than vanilla policy gradients. The core idea is to ensure stable updates by clipping the policy objective and using multiple epochs of mini-batch updates.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Now, I want to know the difference between PPO reinforcement learning and Quantum contextuality'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_EG8wX9mC7fGbkCy9VTVipBla', function=Function(arguments='{\"query\": \"Difference between PPO reinforcement learning and Quantum contextuality\"}', name='doc_retrievalaugmented'), type='function'), ChatCompletionMessageToolCall(id='call_vz2egpojISU88GFhMcvg6Vuh', function=Function(arguments='{\"query\": \"What is Quantum contextuality?\"}', name='doc_retrievalaugmented'), type='function')]),\n",
       " {'tool_call_id': 'call_EG8wX9mC7fGbkCy9VTVipBla',\n",
       "  'role': 'tool',\n",
       "  'name': 'doc_retrievalaugmented',\n",
       "  'content': '### Core Argument\\n- **Quantum State Discrimination**: The paper addresses the challenge of quantum state discrimination in noisy channels, which is essential for various quantum information processing tasks.\\n- **Indefinite Causal Order**: It introduces the concept of indefinite causal order using a quantum switch to enhance the discrimination process.\\n- **Superswitches**: Higher-order generalizations of the quantum switch, termed \"superswitches,\" are proposed to further improve performance.\\n\\n### Evidence\\n- **Quantum Switch**: The quantum switch superposes the sequential action of two quantum channels, providing advantages in tasks where standard operations fail.\\n- **Superswitches**: These higher-order constructs can improve guessing probabilities beyond what is achievable with single- and multi-copy state discrimination.\\n- **Performance Metrics**: The study measures the guessing probability for various channels and ensembles, showing significant enhancements with the use of quantum switches and superswitches.\\n\\n### Conclusions\\n- **Enhanced Guessing Probability**: The use of quantum switches significantly improves the guessing probability for certain channels and ensembles.\\n- **Practical Implementation**: The protocol often eliminates the need for redesigning optimal measurements, making it practical for real-world applications.\\n- **Superswitch Superiority**: Superswitches outperform standard quantum switches in specific scenarios, especially for general Pauli channels and higher state-space dimensions.\\n\\n### User Query: Difference between PPO Reinforcement Learning and Quantum Contextuality\\n- **PPO Reinforcement Learning**:\\n  - **Core Argument**: Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed to balance exploration and exploitation by optimizing a policy within a trust region.\\n  - **Evidence**: PPO uses clipped probability ratios to ensure stable and efficient policy updates, often demonstrated through empirical performance on various benchmarks.\\n  - **Conclusions**: PPO is effective for training agents in complex environments, providing a robust and scalable approach to reinforcement learning.\\n\\n- **Quantum Contextuality**:\\n  - **Core Argument**: Quantum contextuality refers to the phenomenon where the outcome of a measurement cannot be explained by any non-contextual hidden variable theory, highlighting the non-classical nature of quantum mechanics.\\n  - **Evidence**: Demonstrated through experiments and theoretical models, quantum contextuality shows that measurement outcomes depend on the context, i.e., the set of compatible measurements.\\n  - **Conclusions**: Quantum contextuality is a fundamental feature of quantum mechanics, with implications for quantum computation and information theory, distinguishing it from classical systems.\\n\\nIn summary, PPO reinforcement learning focuses on optimizing policies for decision-making in uncertain environments, while quantum contextuality deals with the inherent non-classical behavior of quantum measurements. The two concepts belong to different domains—machine learning and quantum mechanics, respectively—and address distinct types of problems.'},\n",
       " {'tool_call_id': 'call_vz2egpojISU88GFhMcvg6Vuh',\n",
       "  'role': 'tool',\n",
       "  'name': 'doc_retrievalaugmented',\n",
       "  'content': '### Core Argument\\n- The paper \"Enhancing Quantum State Discrimination with Indefinite Causal Order\" by Spiros Kechrimparis et al. argues that the use of indefinite causal order, particularly through quantum switches and their higher-order generalizations (superswitches), can significantly improve the performance of quantum state discrimination in noisy and unknown channels.\\n\\n### Evidence\\n- **Quantum State Discrimination Problem**:\\n  - Quantum state discrimination is essential for various quantum technologies, including communication, cryptography, and machine learning.\\n  - The challenge lies in decoding information encoded in quantum states through noisy and often unknown channels.\\n\\n- **Indefinite Causal Order and Quantum Switch**:\\n  - The concept of indefinite causal order is introduced using a quantum switch, which superposes the sequential action of two quantum channels, making the order of channel application indeterminate.\\n  - Quantum switches have demonstrated advantages in tasks that are unachievable with standard operations.\\n\\n- **Superswitches**:\\n  - Superswitches, higher-order generalizations of the quantum switch, are defined and shown to further improve guessing probabilities in certain scenarios.\\n  - They outperform standard quantum switches, especially for general Pauli channels and higher state-space dimensions.\\n\\n- **Results and Findings**:\\n  - The use of quantum switches significantly enhances guessing probabilities for certain channels and ensembles compared to traditional state discrimination methods.\\n  - The protocol maintains optimal discrimination without the need for redesigning the optimal measurement, even with unknown noisy states.\\n\\n### Conclusions\\n- Indefinite causal order, implemented through quantum switches and superswitches, offers a substantial improvement in quantum state discrimination tasks.\\n- This approach is particularly effective in noisy and unknown channels, providing a robust solution without the need for complex redesigns of measurement strategies.\\n- The findings suggest that leveraging indefinite causal order could be a pivotal advancement in the practical application of quantum technologies.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_conversation.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c244b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
